# ML Pipeline for Security Risk Analysis

A machine learning pipeline built with FastAPI, scikit-learn, and SQLite to classify security events as "safe" or "risky" based on log data. The pipeline processes a synthetic dataset, trains a Logistic Regression model, exposes predictions via an API, logs results to SQLite, and is containerized with Docker for cloud-native deployment.

# Objective

Classify security events as "safe" (0) or "risky" (1) using request_size and response_time from log data, simulating the "Pentest for GenAI" aspect of the Senior Software Engineer (Backend) role at MATVIS.

# Project Structure

- `main.py`: FastAPI application with the `/predict` endpoint.
- `train_model.py`: Generates synthetic dataset, trains the model, and saves it.
- `database.py`: SQLite setup and operations for logging predictions.
- `models.py`: Pydantic model for input validation.
- `Dockerfile`: Docker configuration for containerization.
- `requirements.txt`: Python dependencies.
- `data/security_logs.csv`: Synthetic dataset (generated by `train_model.py`).
- `model.pkl`: Trained Logistic Regression model.
- `scaler.pkl`: Trained StandardScaler for preprocessing.

# Prerequisites

- **Docker**: Installed and running for containerized deployment.
- **Python 3.9+**: Optional, for local testing without Docker.
- **curl or Postman**: For testing the API endpoint.

# Setup Instructions

## Local Setup (Without Docker)

1. Create a project directory and save the required files: `main.py`, `train_model.py`, `database.py`, `models.py`, `requirements.txt`.
2. Create a `data` directory for `security_logs.csv`.
3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```
4. Train the model and generate the dataset:
   ```bash
   python train_model.py
   ```
5. Run the FastAPI application:
   ```bash
   uvicorn main:app --reload
   ```
6. Access the API at `http://localhost:8000`.

## Docker Setup

1. Save all files in a project directory, including a `data` directory for `security_logs.csv`.
2. Train the model to generate `model.pkl`, `scaler.pkl`, and `data/security_logs.csv`:
   ```bash
   python train_model.py
   ```
3. Build the Docker image:
   ```bash
   docker build -t security-risk-api .
   ```
4. Run the Docker container with volumes for SQLite and model persistence:
   ```bash
   docker run -d -p 8000:8000 -v $(pwd)/predictions.db:/app/predictions.db -v $(pwd)/model.pkl:/app/model.pkl -v $(pwd)/scaler.pkl:/app/scaler.pkl security-risk-api
   ```
5. Access the API at `http://localhost:8000`.

# Testing the API

## Predict Risk (POST /predict)

```bash
curl -X POST "http://localhost:8000/predict" \
-H "Content-Type: application/json" \
-d '{"request_size": 1500, "response_time": 800}'
```

**Expected Response** (example):
```json
{"event_id": "evt_1741038540", "is_risky": 1}
```

## Verify Predictions in SQLite

```bash
sqlite3 predictions.db "SELECT * FROM predictions"
```

**Expected Output** (example):
```
evt_1741038540|1500.0|800.0|1|2025-05-29T20:29:00.123456Z
```

# Dataset

The synthetic dataset (`data/security_logs.csv`) contains 100 rows with columns:

- `event_id`: Unique identifier (e.g., `evt_001`).
- `request_size`: Random integer between 100 and 2000 (bytes).
- `response_time`: Random integer between 50 and 1000 (milliseconds).
- `is_risky`: Binary (0 for safe, 1 for risky), based on `request_size > 1000` or `response_time > 500`.

**Sample Rows**:
```
event_id,request_size,response_time,is_risky
evt_001,1567,234,1
evt_002,432,678,1
evt_003,789,123,0
...
```

# Implementation Details

- **ML Pipeline**: Uses scikit-learnâ€™s Logistic Regression for binary classification, with StandardScaler for normalizing `request_size` and `response_time`.
- **FastAPI**: The `/predict` endpoint accepts `request_size` and `response_time`, applies scaling, and returns predictions with a generated `event_id`.
- **SQLite**: Logs predictions with `event_id`, `request_size`, `response_time`, `is_risky`, and `timestamp`.
- **Docker**: Containerizes the app, model, and scaler, with volumes for persistent SQLite storage (`predictions.db`).
- **Security**: Pydantic validates numeric inputs, and error handling catches preprocessing or database issues.

# Potential Optimizations

- **Large Datasets**: Use Apache Spark for distributed preprocessing if the dataset grows significantly.
- **Model Choice**: Switch to Random Forest for better accuracy on complex data, at the cost of slower inference.
- **Performance**: Implement Redis caching for frequent predictions or use `aiosqlite` for non-blocking database writes.
- **Scalability**: Deploy with Kubernetes for auto-scaling under high traffic.

# Troubleshooting

- **Model Not Found**: Ensure `model.pkl` and `scaler.pkl` are generated by running `train_model.py` before starting the API.
- **Database Errors**: Verify `predictions.db` is writable in the Docker volume.
- **Port Conflicts**: Check that port `8000` is free or modify the Docker run command.
- **Prediction Errors**: Ensure input values are valid numbers to avoid preprocessing failures.

# Interview Preparation Tips

- **Explain Trade-offs**: Discuss why Logistic Regression was chosen (fast, interpretable) vs. Random Forest (more accurate, slower).
- **Optimization for Scale**: Highlight using Spark for large datasets or caching for frequent predictions.
- **Security**: Emphasize input validation and potential API key authentication for enhanced security.
- **Docker Deployment**: Be ready to explain the Dockerfile and volume setup for containerization.
- **Mentoring**: Structure explanations clearly, as if guiding a junior developer, to align with mentoring expectations.